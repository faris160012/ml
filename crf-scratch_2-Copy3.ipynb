{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a07bb1",
   "metadata": {},
   "source": [
    "# use python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dcadcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    # Load the training and test data\n",
    "    # Reads a dataset in the CoNLL format from a file\n",
    "    # The format is one token per line:\n",
    "    # [word] [POS] [syntactic chunk] *potential junk column* [NER tag]\n",
    "    # One blank line appears after each sentence\n",
    "    train = read_data(\"data/eng.train\")\n",
    "    dev = read_data(\"data/eng.testa\")\n",
    "    # Here's a few sentences...\n",
    "    # print \"Examples of sentences:\"\n",
    "    # print str(dev[1])\n",
    "    # print str(dev[3])\n",
    "    # print str(dev[5])\n",
    "    system_to_run = 'CRF'\n",
    "    # Set to True when you're ready to run your CRF on the test set to produce the final output\n",
    "    run_on_test = True\n",
    "    # Train our model\n",
    "    if system_to_run == \"BAD\":\n",
    "        bad_model = train_bad_ner_model(train)\n",
    "        dev_decoded = [bad_model.decode(test_ex) for test_ex in dev]\n",
    "    elif system_to_run == \"HMM\":\n",
    "        hmm_model = train_hmm_model(train)\n",
    "        dev_decoded = [hmm_model.decode(test_ex) for test_ex in dev]\n",
    "    elif system_to_run == \"CRF\":\n",
    "        crf_model = train_crf_model(train, 1, 0.1, weights_file='', output_weights='')\n",
    "        dev_decoded = [crf_model.decode(test_ex) for test_ex in dev]\n",
    "        if run_on_test:\n",
    "            test = read_data(\"data/eng.testb.blind\")\n",
    "            test_decoded = [crf_model.decode(test_ex) for test_ex in test]\n",
    "            print_output(test_decoded, \"eng.testb.out\")\n",
    "    else:\n",
    "        raise Exception(\"Pass in either BAD, HMM, or CRF to run the appropriate system\")\n",
    "    # Print the evaluation statistics\n",
    "    print_evaluation(dev, dev_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68b988e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features\n",
      "Ex 0/14934\n",
      "Ex 500/14934\n",
      "Ex 1000/14934\n",
      "Ex 1500/14934\n",
      "Ex 2000/14934\n",
      "Ex 2500/14934\n",
      "Ex 3000/14934\n",
      "Ex 3500/14934\n",
      "Ex 4000/14934\n",
      "Ex 4500/14934\n",
      "Ex 5000/14934\n",
      "Ex 5500/14934\n",
      "Ex 6000/14934\n",
      "Ex 6500/14934\n",
      "Ex 7000/14934\n",
      "Ex 7500/14934\n",
      "Ex 8000/14934\n",
      "Ex 8500/14934\n",
      "Ex 9000/14934\n",
      "Ex 9500/14934\n",
      "Ex 10000/14934\n",
      "Ex 10500/14934\n",
      "Ex 11000/14934\n",
      "Ex 11500/14934\n",
      "Ex 12000/14934\n",
      "Ex 12500/14934\n",
      "Ex 13000/14934\n",
      "Ex 13500/14934\n",
      "Ex 14000/14934\n",
      "Ex 14500/14934\n",
      "Initital Statistics\n",
      "Epoch 1\n",
      "Training on 0\n",
      "Training on 500\n",
      "Training on 1000\n",
      "Training on 1500\n",
      "Training on 2000\n",
      "Training on 2500\n",
      "Training on 3000\n",
      "Training on 3500\n",
      "Training on 4000\n",
      "Training on 4500\n",
      "Training on 5000\n",
      "Training on 5500\n",
      "Training on 6000\n",
      "Training on 6500\n",
      "Training on 7000\n",
      "Training on 7500\n",
      "Training on 8000\n",
      "Training on 8500\n",
      "Training on 9000\n",
      "Training on 9500\n",
      "Training on 10000\n",
      "Training on 10500\n",
      "Training on 11000\n",
      "Training on 11500\n",
      "Training on 12000\n",
      "Training on 12500\n",
      "Training on 13000\n",
      "Training on 13500\n",
      "Training on 14000\n",
      "Training on 14500\n",
      "Labeled F1: 72.74, precision: 4149/5661 = 73.29, recall: 4149/5746 = 72.21\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a910ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads a dataset in the CoNLL format from a file\n",
    "# The format is one token per line:\n",
    "# [word] [POS] [syntactic chunk] *potential junk column* [NER tag]\n",
    "# One blank line appears after each sentence\n",
    "def read_data(file):\n",
    "    f = open(file)\n",
    "    sentences = []\n",
    "    curr_tokens = []\n",
    "    curr_bio_tags = []\n",
    "    for line in f:\n",
    "        stripped = line.strip()\n",
    "        if stripped != \"\":\n",
    "            fields = stripped.split(\" \")\n",
    "            if len(fields) == 4 or len(fields) == 5:\n",
    "                # TODO: Modify this line to remember POS tags (fields[1]) or chunks (fields[2]) if desired\n",
    "                curr_tokens.append(Token(fields[0], fields[1], fields[2]))\n",
    "                # N.B. fields[-1] because there are weird extra fields in .train and .testa\n",
    "                curr_bio_tags.append(fields[-1])\n",
    "        elif stripped == \"\" and len(curr_tokens) > 0:\n",
    "            sentences.append(LabeledSentence(curr_tokens, chunks_from_bio_tag_seq(curr_bio_tags)))\n",
    "            curr_tokens = []\n",
    "            curr_bio_tags = []\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f009d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstraction to bundle words with POS and chunks for featurization\n",
    "class Token:\n",
    "    def __init__(self, word, pos, chunk):\n",
    "        self.word = word\n",
    "        self.pos = pos\n",
    "        self.chunk = chunk\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.word\n",
    "    \n",
    "# Thin wrapper over a sequence of Tokens representing a sentence and an optional set of chunks\n",
    "# representation NER labels, which are also stored as BIO tags\n",
    "class LabeledSentence:\n",
    "    def __init__(self, tokens, chunks=None):\n",
    "        self.tokens = tokens\n",
    "        self.chunks = chunks\n",
    "        if chunks is None:\n",
    "            self.bio_tags = None\n",
    "        else:\n",
    "            self.bio_tags = bio_tags_from_chunks(\n",
    "                self.chunks, \n",
    "                len(self.tokens))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr([repr(tok) for tok in self.tokens]) + \"\\n\" + repr([repr(chunk) for chunk in self.chunks])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def get_bio_tags(self):\n",
    "        return self.bio_tags\n",
    "\n",
    "# Convert BIO tags to (start, end, label) chunk representations\n",
    "# (start, end) are semi-inclusive, meaning that in the sentence\n",
    "# He met Barack Obama yesterday\n",
    "# Barack Obama has the span (2, 4)\n",
    "# N.B. this method only works because chunks are non-overlapping in this data\n",
    "def chunks_from_bio_tag_seq(bio_tags):\n",
    "    chunks = []\n",
    "    curr_tok_start = -1\n",
    "    curr_tok_label = \"\"\n",
    "    for idx, tag in enumerate(bio_tags):\n",
    "        if isB(tag):\n",
    "            label = get_tag_label(tag)\n",
    "            if curr_tok_label != \"\":\n",
    "                chunks.append(Chunk(curr_tok_start, idx, curr_tok_label))\n",
    "            curr_tok_label = label\n",
    "            curr_tok_start = idx\n",
    "        elif isI(tag):\n",
    "            label = get_tag_label(tag)\n",
    "            # if label != curr_tok_label:\n",
    "                # print \"WARNING: invalid tag sequence (I after O); ignoring the I: \" + repr(bio_tags)\n",
    "        else: # isO(tag):\n",
    "            if curr_tok_label != \"\":\n",
    "                chunks.append(Chunk(curr_tok_start, idx, curr_tok_label))\n",
    "            curr_tok_label = \"\"\n",
    "            curr_tok_start = -1\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf98d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a chunk representation back to BIO tags\n",
    "def bio_tags_from_chunks(chunks, sent_len):\n",
    "    tags = []\n",
    "    for i in xrange(0, sent_len):\n",
    "        matching_chunks = filter(lambda chunk: chunk.start_idx <= i and i < chunk.end_idx, chunks)\n",
    "        if len(matching_chunks) > 0:\n",
    "            if i == matching_chunks[0].start_idx:\n",
    "                tags.append(\"B-\" + matching_chunks[0].label)\n",
    "            else:\n",
    "                tags.append(\"I-\" + matching_chunks[0].label)\n",
    "        else:\n",
    "            tags.append(\"O\")\n",
    "    return tags\n",
    "\n",
    "def isB(ner_tag):\n",
    "    return ner_tag.startswith(\"B\")\n",
    "\n",
    "\n",
    "def isI(ner_tag):\n",
    "    return ner_tag.startswith(\"I\")\n",
    "\n",
    "\n",
    "def isO(ner_tag):\n",
    "    return ner_tag == \"O\"\n",
    "\n",
    "# Thin wrapper around a start and end index coupled with a label, representing,\n",
    "# e.g., a chunk PER over the span (3,5). Indices are semi-inclusive, so (3,5)\n",
    "# contains tokens 3 and 4 (0-based indexing).\n",
    "class Chunk:\n",
    "    def __init__(self, start_idx, end_idx, label):\n",
    "        self.start_idx = start_idx\n",
    "        self.end_idx = end_idx\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"(\" + repr(self.start_idx) + \", \" + repr(self.end_idx) + \", \" + self.label + \")\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.start_idx == other.start_idx and self.end_idx == other.end_idx and self.label == other.label\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.start_idx) + hash(self.end_idx) + hash(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e8035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_crf_model(sentences, epochs, lr, weights_file=\"\", output_weights=\"\"):\n",
    "    tag_indexer = Indexer()\n",
    "    for sentence in sentences:\n",
    "        for tag in sentence.get_bio_tags():\n",
    "            tag_indexer.get_index(tag)\n",
    "    transition_mat = np.ones((len(tag_indexer), len(tag_indexer)))\n",
    "    for tag_idxa in range(0, len(tag_indexer)):\n",
    "        for tag_idxb in range(0, len(tag_indexer)):\n",
    "            tag_a = tag_indexer.get_object(tag_idxa)\n",
    "            tag_b = tag_indexer.get_object(tag_idxb)\n",
    "            if isI(tag_b) and (get_tag_label(tag_b) != get_tag_label(tag_a)):\n",
    "                transition_mat[tag_idxa][tag_idxb] = 0\n",
    "    \n",
    "    \n",
    "    print \"Extracting features\"\n",
    "    \n",
    "    \n",
    "    feature_indexer = Indexer()\n",
    "    # 4-d list indexed by sentence index, word index, tag index, feature index\n",
    "    feature_cache = [\n",
    "        [\n",
    "            [\n",
    "                [] for k in xrange(0, len(tag_indexer))\n",
    "            ] for j in xrange(0, len(sentences[i]))\n",
    "        ] for i in xrange(0, len(sentences))\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    for sentence_idx in xrange(0, len(sentences)):\n",
    "        if sentence_idx % 500 == 0:\n",
    "            print(\"Ex \" + repr(sentence_idx) + \"/\" + repr(len(sentences)))\n",
    "        for word_idx in xrange(0, len(sentences[sentence_idx])):\n",
    "            for tag_idx in xrange(0, len(tag_indexer)):\n",
    "                feature_cache[sentence_idx][word_idx][tag_idx] = extract_emission_features(\n",
    "                    sentences[sentence_idx], \n",
    "                    word_idx, tag_indexer.get_object(tag_idx), \n",
    "                    feature_indexer, \n",
    "                    add_to_indexer=True)\n",
    "                \n",
    "    \n",
    "    feature_weights = np.random.rand((len(feature_indexer)))\n",
    "    if weights_file != \"\":\n",
    "        feature_weights = np.load(weights_file)\n",
    "\n",
    "    print(\"Initital Statistics\")\n",
    "    model = CrfNerModel(tag_indexer, feature_indexer, feature_weights)\n",
    "    # TODO : currently using only emission features, also extend to transition features if possible\n",
    "    batch_size = 1\n",
    "    # training loop\n",
    "    for epoch in range(0, epochs):\n",
    "        print(\"Epoch %d\" % (epoch+1))\n",
    "        gradient = Counter()\n",
    "        for sentence_idx in range(0, len(sentences)):\n",
    "            if sentence_idx%500 == 0:\n",
    "                print('Training on ' + repr(sentence_idx))\n",
    "            log_marginal_probs = compute_log_marginals(sentences[sentence_idx], \n",
    "                                                       tag_indexer, \n",
    "                                                       feature_cache[sentence_idx], \n",
    "                                                       model.feature_weights)\n",
    "\n",
    "            for word_idx in range(0, len(sentences[sentence_idx])):\n",
    "                for tag_idx in range(0, len(tag_indexer)):\n",
    "                    gradient.increment_all(\n",
    "                        feature_cache[sentence_idx][word_idx][tag_idx], \n",
    "                        - np.exp(log_marginal_probs[word_idx][tag_idx])\n",
    "                    )\n",
    "                \n",
    "                gold_tag = sentences[sentence_idx].get_bio_tags()[word_idx]\n",
    "                gold_tag_idx = tag_indexer.index_of(gold_tag)\n",
    "                gradient.increment_all(feature_cache[sentence_idx][word_idx][gold_tag_idx], 1.0)\n",
    "                \n",
    "            if (sentence_idx+1) % batch_size == 0:\n",
    "                for weight_idx in gradient.keys():\n",
    "                    model.feature_weights[weight_idx] += (lr * gradient.get_count(weight_idx))/batch_size\n",
    "                gradient = Counter()\n",
    "    np.save(output_weights, model.feature_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b6df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer(object):\n",
    "    def __init__(self):\n",
    "        self.objs_to_ints = {}\n",
    "        self.ints_to_objs = {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str([str(self.get_object(i)) for i in xrange(0, len(self))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.objs_to_ints)\n",
    "\n",
    "    def get_object(self, index):\n",
    "        if (index not in self.ints_to_objs):\n",
    "            return None\n",
    "        else:\n",
    "            return self.ints_to_objs[index]\n",
    "\n",
    "    def contains(self, object):\n",
    "        return self.index_of(object) != -1\n",
    "\n",
    "    # Returns -1 if the object isn't present, index otherwise\n",
    "    def index_of(self, object):\n",
    "        if (object not in self.objs_to_ints):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.objs_to_ints[object]\n",
    "\n",
    "    # Adds the object to the index if it isn't present, always returns a nonnegative index\n",
    "    def get_index(self, object, add=True):\n",
    "        if not add:\n",
    "            return self.index_of(object)\n",
    "        if (object not in self.objs_to_ints):\n",
    "            new_idx = len(self.objs_to_ints)\n",
    "            self.objs_to_ints[object] = new_idx\n",
    "            self.ints_to_objs[new_idx] = object\n",
    "        return self.objs_to_ints[object]\n",
    "    \n",
    "# Gets the label component of the NER tag: e.g., returns PER for B-PER\n",
    "def get_tag_label(ner_tag):\n",
    "    if len(ner_tag) > 2:\n",
    "        return ner_tag[2:]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def extract_emission_features(sentence, word_index, tag, feature_indexer, add_to_indexer):\n",
    "    feats = []\n",
    "    curr_word = sentence.tokens[word_index].word\n",
    "    # Lexical and POS features on this word, the previous, and the next (Word-1, Word0, Word1)\n",
    "    for idx_offset in xrange(-1, 2):\n",
    "        if word_index + idx_offset < 0:\n",
    "            active_word = \"<s>\"\n",
    "        elif word_index + idx_offset >= len(sentence):\n",
    "            active_word = \"</s>\"\n",
    "        else:\n",
    "            active_word = sentence.tokens[word_index + idx_offset].word\n",
    "        if word_index + idx_offset < 0:\n",
    "            active_pos = \"<S>\"\n",
    "        elif word_index + idx_offset >= len(sentence):\n",
    "            active_pos = \"</S>\"\n",
    "        else:\n",
    "            active_pos = sentence.tokens[word_index + idx_offset].pos\n",
    "        maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":Word\" + repr(idx_offset) + \"=\" + active_word)\n",
    "        maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":Pos\" + repr(idx_offset) + \"=\" + active_pos)\n",
    "    # Character n-grams of the current word\n",
    "    max_ngram_size = 3\n",
    "    for ngram_size in xrange(1, max_ngram_size+1):\n",
    "        start_ngram = curr_word[0:min(ngram_size, len(curr_word))]\n",
    "        maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":StartNgram=\" + start_ngram)\n",
    "        end_ngram = curr_word[max(0, len(curr_word) - ngram_size):]\n",
    "        maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":EndNgram=\" + end_ngram)\n",
    "    # Look at a few word shape features\n",
    "    maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":IsCap=\" + repr(curr_word[0].isupper()))\n",
    "    maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":IsAllCap=\" + repr(curr_word.isupper()))\n",
    "    maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":dashExists=\" + repr(\"-\" in curr_word))\n",
    "    # Compute word shape\n",
    "    new_word = []\n",
    "    for i in xrange(0, len(curr_word)):\n",
    "        if curr_word[i].isupper():\n",
    "            new_word += \"X\"\n",
    "        elif curr_word[i].islower():\n",
    "            new_word += \"x\"\n",
    "        elif curr_word[i].isdigit():\n",
    "            new_word += \"0\"\n",
    "        else:\n",
    "            new_word += \"?\"\n",
    "    maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":WordShape=\" + repr(new_word))\n",
    "    return np.asarray(feats, dtype=int)\n",
    "\n",
    "\n",
    "class CrfNerModel(object):\n",
    "    def __init__(self, tag_indexer, feature_indexer, feature_weights):\n",
    "        self.tag_indexer = tag_indexer\n",
    "        self.feature_indexer = feature_indexer\n",
    "        self.feature_weights = feature_weights\n",
    "\n",
    "    # Takes a LabeledSentence object and returns a new copy of that sentence with a set of chunks predicted by\n",
    "    # the CRF model. See BadNerModel for an example implementation\n",
    "    def decode(self, sentence):\n",
    "        feature_cache = [[[] for k in xrange(0, len(self.tag_indexer))] for j in xrange(0, len(sentence))]\n",
    "        for word_idx in range(0, len(sentence)):\n",
    "            for tag_idx in range(0, len(self.tag_indexer)):\n",
    "                feature_cache[word_idx][tag_idx] = extract_emission_features(\n",
    "                    sentence, word_idx, \n",
    "                    self.tag_indexer.get_object(tag_idx), \n",
    "                    self.feature_indexer, \n",
    "                    add_to_indexer=False)\n",
    "\n",
    "                \n",
    "        # Viterbi\n",
    "        score = np.zeros((len(sentence), len(self.tag_indexer)))\n",
    "        back_pointers = np.ones((len(sentence), len(self.tag_indexer))) * -1\n",
    "        sequence_scorer = FeatureBasedSequenceScorer(self.tag_indexer, self.feature_indexer, self.feature_weights)\n",
    "        for word_idx in xrange(0, len(sentence)):\n",
    "            if word_idx == 0:\n",
    "                for tag_idx in xrange(0, len(self.tag_indexer)):\n",
    "                    tag = self.tag_indexer.get_object(tag_idx)\n",
    "                    if isI(tag):\n",
    "                        score[word_idx][tag_idx] = -np.inf\n",
    "                    else:    \n",
    "                        score[word_idx][tag_idx] = sequence_scorer.score_init(feature_cache, tag_idx)\n",
    "            else:\n",
    "                for curr_tag_idx in xrange(0, len(self.tag_indexer)):\n",
    "                    score[word_idx][curr_tag_idx] = -np.inf\n",
    "                    for prev_tag_idx in xrange(0, len(self.tag_indexer)):\n",
    "                        # TODO : did not prohibit the O-I transition at the last word\n",
    "                        curr_tag = self.tag_indexer.get_object(curr_tag_idx)\n",
    "                        prev_tag = self.tag_indexer.get_object(prev_tag_idx)\n",
    "                        if isO(prev_tag) and isI(curr_tag):\n",
    "                            continue\n",
    "                        if isI(curr_tag) and (get_tag_label(curr_tag) != get_tag_label(prev_tag)):\n",
    "                            continue\n",
    "                        curr_score = sequence_scorer.score_transition(feature_cache, prev_tag_idx, curr_tag_idx) + \\\n",
    "                                        sequence_scorer.score_emission(feature_cache, curr_tag_idx, word_idx) + score[word_idx-1][prev_tag_idx]\n",
    "                        if curr_score > score[word_idx][curr_tag_idx]:\n",
    "                            score[word_idx][curr_tag_idx] = curr_score\n",
    "                            back_pointers[word_idx][curr_tag_idx] = prev_tag_idx\n",
    "                            \n",
    "        \n",
    "        max_score_idx = score.argmax(axis=1)[-1]\n",
    "        idx = max_score_idx\n",
    "        pred_tags = []\n",
    "        word_idx = len(sentence) - 1\n",
    "        \n",
    "        while idx != -1 :\n",
    "            pred_tags.append(self.tag_indexer.get_object(idx))\n",
    "            idx = back_pointers[int(word_idx)][int(idx)]\n",
    "            word_idx -= 1\n",
    "        pred_tags.reverse()\n",
    "        \n",
    "        return LabeledSentence(sentence.tokens, chunks_from_bio_tag_seq(pred_tags))\n",
    "    \n",
    "    class Counter(object):\n",
    "    def __init__(self):\n",
    "        self.counter = {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str([str(key) + \": \" + str(self.get_count(key)) for key in self.counter.keys()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.counter)\n",
    "\n",
    "    def keys(self):\n",
    "        return self.counter.keys()\n",
    "\n",
    "    def get_count(self, key):\n",
    "        if self.counter.has_key(key):\n",
    "            return self.counter[key]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def increment_count(self, obj, count):\n",
    "        if self.counter.has_key(obj):\n",
    "            self.counter[obj] = self.counter[obj] + count\n",
    "        else:\n",
    "            self.counter[obj] = count\n",
    "\n",
    "    def increment_all(self, objs_list, count):\n",
    "        for obj in objs_list:\n",
    "            self.increment_count(obj, count)\n",
    "\n",
    "    def set_count(self, obj, count):\n",
    "        self.counter[obj] = count\n",
    "\n",
    "    def add(self, otherCounter):\n",
    "        for key in otherCounter.counter.keys():\n",
    "            self.increment_count(key, otherCounter.counter[key])\n",
    "\n",
    "    # Bad O(n) implementation right now\n",
    "    def argmax(self):\n",
    "        best_key = None\n",
    "        for key in self.counter.keys():\n",
    "            if best_key is None or self.get_count(key) > self.get_count(best_key):\n",
    "                best_key = key\n",
    "        return best_key\n",
    "    \n",
    "    # TODO : implementation specific to emission features only, change forward and backward if adding transition features\n",
    "def compute_log_marginals(sentence, tag_indexer, sentence_feature_cache, feature_weights):\n",
    "    # find alpha -> forward pass\n",
    "    log_alpha = np.zeros((len(sentence), len(tag_indexer)))\n",
    "    for tag_idx in xrange(0, len(tag_indexer)):\n",
    "        log_alpha[0][tag_idx] = score_indexed_features(sentence_feature_cache[0][tag_idx], feature_weights)\n",
    "    for word_idx in xrange(1, len(sentence)):\n",
    "        for tag_idx in xrange(0, len(tag_indexer)):\n",
    "            log_alpha[word_idx][tag_idx] = -np.inf\n",
    "            for prev_tag_idx in xrange(0, len(tag_indexer)):\n",
    "                curr_tag = tag_indexer.get_object(tag_idx)\n",
    "                prev_tag = tag_indexer.get_object(prev_tag_idx)\n",
    "                if isI(curr_tag) and get_tag_label(curr_tag) != get_tag_label(prev_tag):\n",
    "                    continue\n",
    "                log_alpha[word_idx][tag_idx] = np.logaddexp(\n",
    "                    log_alpha[word_idx][tag_idx], \\\n",
    "                    log_alpha[word_idx - 1][prev_tag_idx] + \\\n",
    "                    score_indexed_features(sentence_feature_cache[word_idx][tag_idx],\n",
    "                                           feature_weights))\n",
    "             \n",
    "            # log_alpha[word_idx][tag_idx] = scipy.misc.logsumexp(log_alpha[word_idx-1] + score_indexed_features(sentence_feature_cache[word_idx][tag_idx], feature_weights))\n",
    "    \n",
    "    # find beta -> backward pass\n",
    "    log_beta = np.zeros((len(sentence), len(tag_indexer)))\n",
    "    for word_idx in range(len(sentence)-2, -1, -1):\n",
    "        for tag_idx in range(0, len(tag_indexer)):\n",
    "            log_beta[word_idx][tag_idx] = -np.inf\n",
    "            for next_tag_idx in range(0, len(tag_indexer)):\n",
    "                curr_tag = tag_indexer.get_object(tag_idx)\n",
    "                next_tag = tag_indexer.get_object(next_tag_idx)\n",
    "                if isI(next_tag) and get_tag_label(curr_tag) != get_tag_label(next_tag):\n",
    "                    continue\n",
    "                log_beta[word_idx][tag_idx] = np.logaddexp(\n",
    "                    log_beta[word_idx][tag_idx], \\\n",
    "                    log_beta[word_idx + 1][next_tag_idx] + \\\n",
    "                    score_indexed_features(sentence_feature_cache[word_idx][next_tag_idx], \n",
    "                                           feature_weights))\n",
    "            # tmp = np.apply_along_axis(score_indexed_features, 1, sentence_feature_cache[word_idx], feature_weights)\n",
    "            # log_beta[word_idx][tag_idx] = scipy.misc.logsumexp(log_beta[word_idx + 1] + tmp)\n",
    "\n",
    "    # marginal = alpha[word_idx][tag_idx] * beta[word_idx][tag_idx] / Sigma (alpha, beta)\n",
    "    log_marginal_probs = np.zeros((len(sentence), len(tag_indexer)))\n",
    "    log_marginal_probs = log_alpha + log_beta\n",
    "    # denom = np.apply_along_axis(scipy.misc.logsumexp, 1, log_marginal_probs)\n",
    "    # log_marginal_probs -= denom[:, None]\n",
    "    for word_idx in range(0, len(sentence)):\n",
    "        denom = -np.inf\n",
    "        for tag_idx in range(0, len(tag_indexer)):\n",
    "            denom = np.logaddexp(denom, log_marginal_probs[word_idx][tag_idx])\n",
    "        log_marginal_probs[word_idx] -= denom\n",
    "    return log_marginal_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes a string feat using feature_indexer and adds it to feats.\n",
    "# If add_to_indexer is true, that feature is indexed and added even if it is new\n",
    "# If add_to_indexer is false, unseen features will be discarded\n",
    "def maybe_add_feature(feats, feature_indexer, add_to_indexer, feat):\n",
    "    if add_to_indexer:\n",
    "        feats.append(feature_indexer.get_index(feat))\n",
    "    else:\n",
    "        feat_idx = feature_indexer.index_of(feat)\n",
    "        if feat_idx != -1:\n",
    "            feats.append(feat_idx)\n",
    "            \n",
    "class FeatureBasedSequenceScorer(object):\n",
    "    def __init__(self, tag_indexer, feature_indexer, feature_weights):\n",
    "        self.tag_indexer = tag_indexer\n",
    "        self.feature_indexer = feature_indexer\n",
    "        self.feature_weights = feature_weights\n",
    "\n",
    "    def score_init(self, feature_cache, tag_idx):\n",
    "        return score_indexed_features(feature_cache[0][tag_idx], self.feature_weights)\n",
    "\n",
    "    def score_transition(self, feature_cache, prev_tag_idx, curr_tag_idx):\n",
    "        return 0\n",
    "\n",
    "    def score_emission(self, feature_cache, tag_idx, word_idx):\n",
    "        return score_indexed_features(feature_cache[word_idx][tag_idx], self.feature_weights)\n",
    "\n",
    "# Thin wrapper over a sequence of Tokens representing a sentence and an optional set of chunks\n",
    "# representation NER labels, which are also stored as BIO tags\n",
    "class LabeledSentence:\n",
    "    def __init__(self, tokens, chunks=None):\n",
    "        self.tokens = tokens\n",
    "        self.chunks = chunks\n",
    "        if chunks is None:\n",
    "            self.bio_tags = None\n",
    "        else:\n",
    "            self.bio_tags = bio_tags_from_chunks(self.chunks, len(self.tokens))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr([repr(tok) for tok in self.tokens]) + \"\\n\" + repr([repr(chunk) for chunk in self.chunks])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def get_bio_tags(self):\n",
    "        return self.bio_tags\n",
    "    \n",
    "def score_indexed_features(feats, weights):\n",
    "    score = 0.0\n",
    "    # for feat in feats:\n",
    "    #     score += weights[feat]\n",
    "    score = np.take(weights, feats).sum()\n",
    "    return score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

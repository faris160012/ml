{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a07bb1",
   "metadata": {},
   "source": [
    "# use python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5944b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# from nerdata import *\n",
    "# from utils import *\n",
    "# from models import *\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260eced4",
   "metadata": {},
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ade5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING COUNTER\n",
      "{'a': 5, 'b': 3}\n",
      "a\n",
      "b\n",
      "['a: 8', 'c: 4', 'b: 3'] should be ['a: 8', 'c: 4', 'b: 3']\n",
      "TESTING BEAM\n",
      "Should contain b, c, a: Beam([('b', 7), ('c', 6), ('a', 5)])\n",
      "Should contain e, b, f: Beam([('e', 8), ('b', 7), ('f', 6.5)])\n",
      "Should contain b, c, a, d: Beam([('b', 7), ('c', 6), ('a', 5), ('d', 4)])\n",
      "Should contain e, b, f, c, a: Beam([('e', 8), ('b', 7), ('f', 6.5), ('c', 6), ('a', 5)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Bijection between objects and integers starting at 0. Useful for mapping\n",
    "# labels, features, etc. into coordinates of a vector space.\n",
    "class Indexer(object):\n",
    "    def __init__(self):\n",
    "        self.objs_to_ints = {}\n",
    "        self.ints_to_objs = {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str([str(self.get_object(i)) for i in xrange(0, len(self))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.objs_to_ints)\n",
    "\n",
    "    def get_object(self, index):\n",
    "        if (index not in self.ints_to_objs):\n",
    "            return None\n",
    "        else:\n",
    "            return self.ints_to_objs[index]\n",
    "\n",
    "    def contains(self, object):\n",
    "        return self.index_of(object) != -1\n",
    "\n",
    "    # Returns -1 if the object isn't present, index otherwise\n",
    "    def index_of(self, object):\n",
    "        if (object not in self.objs_to_ints):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.objs_to_ints[object]\n",
    "\n",
    "    # Adds the object to the index if it isn't present, always returns a nonnegative index\n",
    "    def get_index(self, object, add=True):\n",
    "        if not add:\n",
    "            return self.index_of(object)\n",
    "        if (object not in self.objs_to_ints):\n",
    "            new_idx = len(self.objs_to_ints)\n",
    "            self.objs_to_ints[object] = new_idx\n",
    "            self.ints_to_objs[new_idx] = object\n",
    "        return self.objs_to_ints[object]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0127aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from objects to doubles that has a default value of 0 for all elements\n",
    "# Relatively inefficient (dictionary-backed); shouldn't be used for anything very large-scale,\n",
    "# instead use an Indexer over the objects and use a numpy array to store the values\n",
    "class Counter(object):\n",
    "    def __init__(self):\n",
    "        self.counter = {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str([str(key) + \": \" + str(self.get_count(key)) for key in self.counter.keys()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.counter)\n",
    "\n",
    "    def keys(self):\n",
    "        return self.counter.keys()\n",
    "\n",
    "    def get_count(self, key):\n",
    "        if self.counter.has_key(key):\n",
    "            return self.counter[key]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def increment_count(self, obj, count):\n",
    "        if self.counter.has_key(obj):\n",
    "            self.counter[obj] = self.counter[obj] + count\n",
    "        else:\n",
    "            self.counter[obj] = count\n",
    "\n",
    "    def increment_all(self, objs_list, count):\n",
    "        for obj in objs_list:\n",
    "            self.increment_count(obj, count)\n",
    "\n",
    "    def set_count(self, obj, count):\n",
    "        self.counter[obj] = count\n",
    "\n",
    "    def add(self, otherCounter):\n",
    "        for key in otherCounter.counter.keys():\n",
    "            self.increment_count(key, otherCounter.counter[key])\n",
    "\n",
    "    # Bad O(n) implementation right now\n",
    "    def argmax(self):\n",
    "        best_key = None\n",
    "        for key in self.counter.keys():\n",
    "            if best_key is None or self.get_count(key) > self.get_count(best_key):\n",
    "                best_key = key\n",
    "        return best_key\n",
    "\n",
    "\n",
    "# Beam data structure. Maintains a list of scored elements like a Counter, but only keeps the top n\n",
    "# elements after every insertion operation. Insertion can sometimes be slow (list is maintained in\n",
    "# sorted order), access is O(1)\n",
    "class Beam(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.elts = []\n",
    "        self.scores = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Beam(\" + repr(self.get_elts_and_scores()) + \")\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.elts)\n",
    "\n",
    "    # Adds the element to the beam with the given score if the beam has room or if the score\n",
    "    # is better than the score of the worst element currently on the beam\n",
    "    def add(self, elt, score):\n",
    "        if len(self.elts) == self.size and score < self.scores[-1]:\n",
    "            # Do nothing because this element is the worst\n",
    "            return\n",
    "        # If the list is empty, just insert the item\n",
    "        if len(self.elts) == 0:\n",
    "            self.elts.insert(0, elt)\n",
    "            self.scores.insert(0, score)\n",
    "        # Otherwise, find the insertion point with binary search\n",
    "        else:\n",
    "            lb = 0\n",
    "            ub = len(self.scores) - 1\n",
    "            # We're searching for the index of the first element with score less than score\n",
    "            while lb < ub:\n",
    "                m = (lb + ub) // 2\n",
    "                # Check > because the list is sorted in descending order\n",
    "                if self.scores[m] > score:\n",
    "                    # Put the lower bound ahead of m because all elements before this are greater\n",
    "                    lb = m + 1\n",
    "                else:\n",
    "                    # m could still be the insertion point\n",
    "                    ub = m\n",
    "            # lb and ub should be equal and indicate the index of the first element with score less than score.\n",
    "            # Might be necessary to insert at the end of the list.\n",
    "            if self.scores[lb] > score:\n",
    "                self.elts.insert(lb + 1, elt)\n",
    "                self.scores.insert(lb + 1, score)\n",
    "            else:\n",
    "                self.elts.insert(lb, elt)\n",
    "                self.scores.insert(lb, score)\n",
    "            # Drop and item from the beam if necessary\n",
    "            if len(self.scores) > self.size:\n",
    "                self.elts.pop()\n",
    "                self.scores.pop()\n",
    "\n",
    "    def get_elts(self):\n",
    "        return self.elts\n",
    "\n",
    "    def get_elts_and_scores(self):\n",
    "        return zip(self.elts, self.scores)\n",
    "\n",
    "    def head(self):\n",
    "        return self.elts[0]\n",
    "\n",
    "\n",
    "# Indexes a string feat using feature_indexer and adds it to feats.\n",
    "# If add_to_indexer is true, that feature is indexed and added even if it is new\n",
    "# If add_to_indexer is false, unseen features will be discarded\n",
    "def maybe_add_feature(feats, feature_indexer, add_to_indexer, feat):\n",
    "    if add_to_indexer:\n",
    "        feats.append(feature_indexer.get_index(feat))\n",
    "    else:\n",
    "        feat_idx = feature_indexer.index_of(feat)\n",
    "        if feat_idx != -1:\n",
    "            feats.append(feat_idx)\n",
    "\n",
    "\n",
    "# Computes the dot product over a list of features (i.e., a sparse feature vector)\n",
    "# and a weight vector (numpy array)\n",
    "def score_indexed_features(feats, weights):\n",
    "    score = 0.0\n",
    "    # for feat in feats:\n",
    "    #     score += weights[feat]\n",
    "    score = np.take(weights, feats).sum()\n",
    "    return score\n",
    "\n",
    "\n",
    "##################\n",
    "# Tests\n",
    "def test_counter():\n",
    "    print \"TESTING COUNTER\"\n",
    "    ctr = Counter()\n",
    "    ctr.increment_count(\"a\", 5)\n",
    "    ctr.increment_count(\"b\", 3)\n",
    "    print str(ctr.counter)\n",
    "    for key in ctr.counter.keys():\n",
    "        print key\n",
    "    ctr2 = Counter()\n",
    "    ctr2.increment_count(\"a\", 3)\n",
    "    ctr2.increment_count(\"c\", 4)\n",
    "    ctr.add(ctr2)\n",
    "    print repr(ctr) + \" should be ['a: 8', 'c: 4', 'b: 3']\"\n",
    "\n",
    "\n",
    "def test_beam():\n",
    "    print \"TESTING BEAM\"\n",
    "    beam = Beam(3)\n",
    "    beam.add(\"a\", 5)\n",
    "    beam.add(\"b\", 7)\n",
    "    beam.add(\"c\", 6)\n",
    "    beam.add(\"d\", 4)\n",
    "    print \"Should contain b, c, a: \" + repr(beam)\n",
    "    beam.add(\"e\", 8)\n",
    "    beam.add(\"f\", 6.5)\n",
    "    print \"Should contain e, b, f: \" + repr(beam)\n",
    "\n",
    "    beam = Beam(5)\n",
    "    beam.add(\"a\", 5)\n",
    "    beam.add(\"b\", 7)\n",
    "    beam.add(\"c\", 6)\n",
    "    beam.add(\"d\", 4)\n",
    "    print \"Should contain b, c, a, d: \" + repr(beam)\n",
    "    beam.add(\"e\", 8)\n",
    "    beam.add(\"f\", 6.5)\n",
    "    print \"Should contain e, b, f, c, a: \" + repr(beam)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_counter()\n",
    "    test_beam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15ff470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nerdata.py\n",
    "\n",
    "\n",
    "# Abstraction to bundle words with POS and chunks for featurization\n",
    "class Token:\n",
    "    def __init__(self, word, pos, chunk):\n",
    "        self.word = word\n",
    "        self.pos = pos\n",
    "        self.chunk = chunk\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.word\n",
    "\n",
    "\n",
    "# Thin wrapper around a start and end index coupled with a label, representing,\n",
    "# e.g., a chunk PER over the span (3,5). Indices are semi-inclusive, so (3,5)\n",
    "# contains tokens 3 and 4 (0-based indexing).\n",
    "class Chunk:\n",
    "    def __init__(self, start_idx, end_idx, label):\n",
    "        self.start_idx = start_idx\n",
    "        self.end_idx = end_idx\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"(\" + repr(self.start_idx) + \", \" + repr(self.end_idx) + \", \" + self.label + \")\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.start_idx == other.start_idx and self.end_idx == other.end_idx and self.label == other.label\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.start_idx) + hash(self.end_idx) + hash(self.label)\n",
    "\n",
    "\n",
    "# Thin wrapper over a sequence of Tokens representing a sentence and an optional set of chunks\n",
    "# representation NER labels, which are also stored as BIO tags\n",
    "class LabeledSentence:\n",
    "    def __init__(self, tokens, chunks=None):\n",
    "        self.tokens = tokens\n",
    "        self.chunks = chunks\n",
    "        if chunks is None:\n",
    "            self.bio_tags = None\n",
    "        else:\n",
    "            self.bio_tags = bio_tags_from_chunks(self.chunks, len(self.tokens))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr([repr(tok) for tok in self.tokens]) + \"\\n\" + repr([repr(chunk) for chunk in self.chunks])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def get_bio_tags(self):\n",
    "        return self.bio_tags\n",
    "\n",
    "\n",
    "# We store NER tags as strings, but they contain two pieces:\n",
    "# a coarse tag type (BIO) and a label (PER), e.g. B-PER\n",
    "def isB(ner_tag):\n",
    "    return ner_tag.startswith(\"B\")\n",
    "\n",
    "\n",
    "def isI(ner_tag):\n",
    "    return ner_tag.startswith(\"I\")\n",
    "\n",
    "\n",
    "def isO(ner_tag):\n",
    "    return ner_tag == \"O\"\n",
    "\n",
    "\n",
    "# Gets the label component of the NER tag: e.g., returns PER for B-PER\n",
    "def get_tag_label(ner_tag):\n",
    "    if len(ner_tag) > 2:\n",
    "        return ner_tag[2:]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Convert BIO tags to (start, end, label) chunk representations\n",
    "# (start, end) are semi-inclusive, meaning that in the sentence\n",
    "# He met Barack Obama yesterday\n",
    "# Barack Obama has the span (2, 4)\n",
    "# N.B. this method only works because chunks are non-overlapping in this data\n",
    "def chunks_from_bio_tag_seq(bio_tags):\n",
    "    chunks = []\n",
    "    curr_tok_start = -1\n",
    "    curr_tok_label = \"\"\n",
    "    for idx, tag in enumerate(bio_tags):\n",
    "        if isB(tag):\n",
    "            label = get_tag_label(tag)\n",
    "            if curr_tok_label != \"\":\n",
    "                chunks.append(Chunk(curr_tok_start, idx, curr_tok_label))\n",
    "            curr_tok_label = label\n",
    "            curr_tok_start = idx\n",
    "        elif isI(tag):\n",
    "            label = get_tag_label(tag)\n",
    "            # if label != curr_tok_label:\n",
    "                # print \"WARNING: invalid tag sequence (I after O); ignoring the I: \" + repr(bio_tags)\n",
    "        else: # isO(tag):\n",
    "            if curr_tok_label != \"\":\n",
    "                chunks.append(Chunk(curr_tok_start, idx, curr_tok_label))\n",
    "            curr_tok_label = \"\"\n",
    "            curr_tok_start = -1\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Converts a chunk representation back to BIO tags\n",
    "def bio_tags_from_chunks(chunks, sent_len):\n",
    "    tags = []\n",
    "    for i in xrange(0, sent_len):\n",
    "        matching_chunks = filter(lambda chunk: chunk.start_idx <= i and i < chunk.end_idx, chunks)\n",
    "        if len(matching_chunks) > 0:\n",
    "            if i == matching_chunks[0].start_idx:\n",
    "                tags.append(\"B-\" + matching_chunks[0].label)\n",
    "            else:\n",
    "                tags.append(\"I-\" + matching_chunks[0].label)\n",
    "        else:\n",
    "            tags.append(\"O\")\n",
    "    return tags\n",
    "\n",
    "\n",
    "# Reads a dataset in the CoNLL format from a file\n",
    "# The format is one token per line:\n",
    "# [word] [POS] [syntactic chunk] *potential junk column* [NER tag]\n",
    "# One blank line appears after each sentence\n",
    "def read_data(file):\n",
    "    f = open(file)\n",
    "    sentences = []\n",
    "    curr_tokens = []\n",
    "    curr_bio_tags = []\n",
    "    for line in f:\n",
    "        stripped = line.strip()\n",
    "        if stripped != \"\":\n",
    "            fields = stripped.split(\" \")\n",
    "            if len(fields) == 4 or len(fields) == 5:\n",
    "                # TODO: Modify this line to remember POS tags (fields[1]) or chunks (fields[2]) if desired\n",
    "                curr_tokens.append(Token(fields[0], fields[1], fields[2]))\n",
    "                # N.B. fields[-1] because there are weird extra fields in .train and .testa\n",
    "                curr_bio_tags.append(fields[-1])\n",
    "        elif stripped == \"\" and len(curr_tokens) > 0:\n",
    "            sentences.append(LabeledSentence(curr_tokens, chunks_from_bio_tag_seq(curr_bio_tags)))\n",
    "            curr_tokens = []\n",
    "            curr_bio_tags = []\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Evaluates the guess sentences with respect to the gold sentences\n",
    "def print_evaluation(gold_sentences, guess_sentences):\n",
    "    correct = 0\n",
    "    num_pred = 0\n",
    "    num_gold = 0\n",
    "    for gold, guess in zip(gold_sentences, guess_sentences):\n",
    "        correct += len(set(guess.chunks) & set(gold.chunks))\n",
    "        num_pred += len(guess.chunks)\n",
    "        num_gold += len(gold.chunks)\n",
    "    if num_pred == 0:\n",
    "        prec = 0\n",
    "    else:\n",
    "        prec = correct/float(num_pred)\n",
    "    if num_gold == 0:\n",
    "        rec = 0\n",
    "    else:\n",
    "        rec = correct/float(num_gold)\n",
    "    if prec == 0 and rec == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * prec * rec / (prec + rec)\n",
    "    print \"Labeled F1: \" + \"{0:.2f}\".format(f1 * 100) +\\\n",
    "          \", precision: \" + repr(correct) + \"/\" + repr(num_pred) + \" = \" + \"{0:.2f}\".format(prec * 100) + \\\n",
    "          \", recall: \" + repr(correct) + \"/\" + repr(num_gold) + \" = \" + \"{0:.2f}\".format(rec * 100)\n",
    "\n",
    "\n",
    "# Writes labeled_sentences to outfile in the CoNLL format\n",
    "def print_output(labeled_sentences, outfile):\n",
    "    f = open(outfile, 'w')\n",
    "    for sentence in labeled_sentences:\n",
    "        bio_tags = sentence.get_bio_tags()\n",
    "        for i in xrange(0, len(sentence)):\n",
    "            tok = sentence.tokens[i]\n",
    "            f.write(tok.word + \" \" + tok.pos + \" \" + tok.chunk + \" \" + bio_tags[i] + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7c5087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adagrad_trainer.py\n",
    "\n",
    "# from utils import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Wrapper for using AdaGrad as the optimizer. AdagradTrainer wraps a weight vector and applies the custom\n",
    "# AdaGrad update using second moments of features to make custom step sizes. This version incorporates L1\n",
    "# regularization: while this regularization should be applied to squash the feature vector on every gradient update,\n",
    "# we instead evaluate the regularizer lazily only when the particular feature is touched (either by gradient update\n",
    "# or by access). approximate lets you turn this off for faster access, but regularization is now applied\n",
    "# somewhat inconsistently.\n",
    "class AdagradTrainer(object):\n",
    "    def __init__(self, init_weights, lamb, eta, approximate=False):\n",
    "        self.weights = init_weights\n",
    "        self.lamb = lamb\n",
    "        self.eta = eta\n",
    "        self.approximate = approximate\n",
    "        self.curr_iter = 0\n",
    "        self.last_iter_touched = [0 for i in xrange(0, self.weights.shape[0])]\n",
    "        self.diag_Gt = np.zeros_like(self.weights, dtype=float)\n",
    "\n",
    "    # Take a sparse representation of the gradient and make an update, normalizing by the batch size to keep\n",
    "    # hyperparameters constant as the batch size is varied\n",
    "    def apply_gradient_update(self, gradient, batch_size):\n",
    "        batch_size_multiplier = 1.0 / batch_size\n",
    "        self.curr_iter += 1\n",
    "        for i in gradient.keys():\n",
    "            xti = self.weights[i]\n",
    "            # N.B.We negate the gradient here because the Adagrad formulas are all for minimizing\n",
    "            # and we're trying to maximize, so think of it as minimizing the negative of the objective\n",
    "            # which has the opposite gradient\n",
    "            # Equation (25) in http://www.cs.berkeley.edu / ~jduchi / projects / DuchiHaSi10.pdf\n",
    "            # eta is the step size, lambda is the regularization\n",
    "            gti = -gradient.get_count(i) * batch_size_multiplier\n",
    "            old_eta_over_Htii = self.eta / (1 + np.sqrt(self.diag_Gt[i]))\n",
    "            self.diag_Gt[i] += gti * gti\n",
    "            Htii = 1 + np.sqrt(self.diag_Gt[i])\n",
    "            eta_over_Htii = self.eta / Htii\n",
    "            new_xti = xti - eta_over_Htii * gti\n",
    "            # Apply the regularizer for every iteration since touched\n",
    "            iters_since_touched = self.curr_iter - self.last_iter_touched[i]\n",
    "            self.last_iter_touched[i] = self.curr_iter\n",
    "            self.weights[i] = np.sign(new_xti) * max(0, np.abs(new_xti) - self.lamb * eta_over_Htii - (iters_since_touched - 1) * self.lamb * old_eta_over_Htii)\n",
    "\n",
    "    # Get the weight of feature i\n",
    "    def access(self, i):\n",
    "        if not self.approximate and self.last_iter_touched[i] != self.curr_iter:\n",
    "            xti = self.weights[i]\n",
    "            Htii = 1 + np.sqrt(self.diag_Gt[i])\n",
    "            eta_over_Htii = self.eta / Htii\n",
    "            iters_since_touched = self.curr_iter - self.last_iter_touched[i]\n",
    "            self.last_iter_touched[i] = self.curr_iter\n",
    "            self.weights[i] = np.sign(xti) * max(0, np.abs(xti) - iters_since_touched * self.lamb * self.eta * eta_over_Htii);\n",
    "        return self.weights[i]\n",
    "\n",
    "    # Score a feature vector\n",
    "    def score(self, feats):\n",
    "        i = 0\n",
    "        score = 0.0\n",
    "        while i < len(feats):\n",
    "            score += self.access(feats[i])\n",
    "            i += 1\n",
    "        return score\n",
    "\n",
    "    # Return the final weight vector values -- manually calls access to force each weight to have an updated value.\n",
    "    def get_final_weights(self):\n",
    "        for i in xrange(0, self.weights.shape[0]):\n",
    "            self.access(i)\n",
    "        return self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c49a9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.py\n",
    "\n",
    "# from nerdata import *\n",
    "# from utils import *\n",
    "# from adagrad_trainer import *\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "\n",
    "# Scoring function for sequence models based on conditional probabilities.\n",
    "# Scores are provided for three potentials in the model: initial scores (applied to the first tag),\n",
    "# emissions, and transitions. Note that CRFs typically don't use potentials of the first type.\n",
    "class ProbabilisticSequenceScorer(object):\n",
    "    def __init__(self, tag_indexer, word_indexer, init_log_probs, transition_log_probs, emission_log_probs):\n",
    "        self.tag_indexer = tag_indexer\n",
    "        self.word_indexer = word_indexer\n",
    "        self.init_log_probs = init_log_probs\n",
    "        self.transition_log_probs = transition_log_probs\n",
    "        self.emission_log_probs = emission_log_probs\n",
    "\n",
    "    def score_init(self, sentence, tag_idx):\n",
    "        return self.init_log_probs[tag_idx]\n",
    "\n",
    "    def score_transition(self, sentence, prev_tag_idx, curr_tag_idx):\n",
    "        return self.transition_log_probs[prev_tag_idx, curr_tag_idx]\n",
    "\n",
    "    def score_emission(self, sentence, tag_idx, word_posn):\n",
    "        word = sentence.tokens[word_posn].word\n",
    "        word_idx = self.word_indexer.index_of(word) if self.word_indexer.contains(word) else self.word_indexer.get_index(\"UNK\")\n",
    "        return self.emission_log_probs[tag_idx, word_idx]\n",
    "\n",
    "\n",
    "class HmmNerModel(object):\n",
    "    def __init__(self, tag_indexer, word_indexer, init_log_probs, transition_log_probs, emission_log_probs):\n",
    "        self.tag_indexer = tag_indexer\n",
    "        self.word_indexer = word_indexer\n",
    "        self.init_log_probs = init_log_probs\n",
    "        self.transition_log_probs = transition_log_probs\n",
    "        self.emission_log_probs = emission_log_probs\n",
    "\n",
    "    # Takes a LabeledSentence object and returns a new copy of that sentence with a set of chunks predicted by\n",
    "    # the HMM model. See BadNerModel for an example implementation\n",
    "    def decode(self, sentence):\n",
    "        score = np.zeros((len(sentence), len(self.tag_indexer)))\n",
    "        back_pointers = np.ones((len(sentence), len(self.tag_indexer))) * -1\n",
    "        sequence_scorer = ProbabilisticSequenceScorer(self.tag_indexer, self.word_indexer, self.init_log_probs, self.transition_log_probs, self.emission_log_probs)\n",
    "        for idx in xrange(0, len(sentence)):\n",
    "            # Initial Scores\n",
    "            if idx == 0:\n",
    "                for tag_idx in xrange(0, len(self.tag_indexer)):    \n",
    "                    score[idx][tag_idx] = sequence_scorer.score_init(sentence, tag_idx) + \\\n",
    "                                            sequence_scorer.score_emission(sentence, tag_idx, idx)\n",
    "            else:\n",
    "                for curr_tag_idx in xrange(0, len(self.tag_indexer)):\n",
    "                    score[idx][curr_tag_idx] = -np.inf\n",
    "                    for prev_tag_idx in xrange(0, len(self.tag_indexer)):\n",
    "                        curr_score = sequence_scorer.score_transition(sentence, prev_tag_idx, curr_tag_idx) + \\\n",
    "                                        sequence_scorer.score_emission(sentence, curr_tag_idx, idx) + score[idx-1][prev_tag_idx]\n",
    "                        if curr_score > score[idx][curr_tag_idx]:\n",
    "                            score[idx][curr_tag_idx] = curr_score\n",
    "                            back_pointers[idx][curr_tag_idx] = prev_tag_idx\n",
    "        max_score_idx = score.argmax(axis=1)[-1]\n",
    "        idx = max_score_idx\n",
    "        pred_tags = []\n",
    "        count = len(sentence) - 1\n",
    "        while idx != -1 :\n",
    "            pred_tags.append(self.tag_indexer.get_object(idx))\n",
    "            idx = back_pointers[count][idx]\n",
    "            count = count - 1\n",
    "        pred_tags.reverse()\n",
    "        return LabeledSentence(sentence.tokens, chunks_from_bio_tag_seq(pred_tags))\n",
    "\n",
    "\n",
    "# Uses maximum-likelihood estimation to read an HMM off of a corpus of sentences.\n",
    "# Any word that only appears once in the corpus is replaced with UNK. A small amount\n",
    "# of additive smoothing is applied to\n",
    "def train_hmm_model(sentences):\n",
    "    # Index words and tags. We do this in advance so we know how big our\n",
    "    # matrices need to be.\n",
    "    tag_indexer = Indexer()\n",
    "    word_indexer = Indexer()\n",
    "    word_indexer.get_index(\"UNK\")\n",
    "    word_counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        for token in sentence.tokens:\n",
    "            word_counter.increment_count(token.word, 1.0)\n",
    "    for sentence in sentences:\n",
    "        for token in sentence.tokens:\n",
    "            # If the word occurs fewer than two times, don't index it -- we'll treat it as UNK\n",
    "            get_word_index(word_indexer, word_counter, token.word)\n",
    "        for tag in sentence.get_bio_tags():\n",
    "            tag_indexer.get_index(tag)\n",
    "    # Count occurrences of initial tags, transitions, and emissions\n",
    "    # Apply additive smoothing to avoid log(0) / infinities / etc.\n",
    "    init_counts = np.ones((len(tag_indexer)), dtype=float) * 0.001\n",
    "    transition_counts = np.ones((len(tag_indexer),len(tag_indexer)), dtype=float) * 0.001\n",
    "    emission_counts = np.ones((len(tag_indexer),len(word_indexer)), dtype=float) * 0.001\n",
    "    for sentence in sentences:\n",
    "        bio_tags = sentence.get_bio_tags()\n",
    "        for i in xrange(0, len(sentence)):\n",
    "            tag_idx = tag_indexer.get_index(bio_tags[i])\n",
    "            word_idx = get_word_index(word_indexer, word_counter, sentence.tokens[i].word)\n",
    "            emission_counts[tag_idx][word_idx] += 1.0\n",
    "            if i == 0:\n",
    "                init_counts[tag_indexer.get_index(bio_tags[i])] += 1.0\n",
    "            else:\n",
    "                transition_counts[tag_indexer.get_index(bio_tags[i-1])][tag_idx] += 1.0\n",
    "    # Turn counts into probabilities for initial tags, transitions, and emissions. All\n",
    "    # probabilities are stored as log probabilities\n",
    "    print repr(init_counts)\n",
    "    init_counts = np.log(init_counts / init_counts.sum())\n",
    "    # transitions are stored as count[prev state][next state], so we sum over the second axis\n",
    "    # and normalize by that to get the right conditional probabilities\n",
    "    transition_counts = np.log(transition_counts / transition_counts.sum(axis=1)[:, np.newaxis])\n",
    "    # similar to transitions\n",
    "    emission_counts = np.log(emission_counts / emission_counts.sum(axis=1)[:, np.newaxis])\n",
    "    print \"Tag indexer: \" + repr(tag_indexer)\n",
    "    print \"Initial state log probabilities: \" + repr(init_counts)\n",
    "    print \"Transition log probabilities: \" + repr(transition_counts)\n",
    "    print \"Emission log probs too big to print...\"\n",
    "    print \"Emission log probs for India: \" + repr(emission_counts[:,word_indexer.get_index(\"India\")])\n",
    "    print \"Emission log probs for Phil: \" + repr(emission_counts[:,word_indexer.get_index(\"Phil\")])\n",
    "    print \"   note that these distributions don't normalize because it's p(word|tag) that normalizes, not p(tag|word)\"\n",
    "    return HmmNerModel(tag_indexer, word_indexer, init_counts, transition_counts, emission_counts)\n",
    "\n",
    "# Implement the forward pass of the algorithm\n",
    "# scored_feature is the phi function(y_t, y_(t-1), x_t)\n",
    "def forward_pass(sentence, tag_indexer, scored_feature):\n",
    "    alpha = np.zeros((len(sentence), len(tag_indexer)))\n",
    "    for tag_idx in xrange(0, len(tag_indexer)):\n",
    "        alpha[0][tag_idx] = scored_feature[0][tag_idx][0]\n",
    "    for word_idx in xrange(0, len(sentence)):\n",
    "        for tag_idx in xrange(0, len(tag_indexer)):\n",
    "            for prev_tag_idx in xrange(0, len(tag_indexer)):\n",
    "                alpha[word_idx][tag_idx] += alpha[word_idx - 1][prev_tag_idx] * scored_feature[tag_idx][prev_tag_idx][word_idx]\n",
    "    return alpha\n",
    "\n",
    "# Implement the backward pass of the algorithm\n",
    "# scored_feature is the phi function(y_t, y_(t-1), x_t)\n",
    "def backward_pass(sentence, tag_indexer, scored_feature):\n",
    "    beta = np.zeros((len(sentence), len(tag_indexer)))\n",
    "    for tag_idx in xrange(0, len(tag_indexer)):\n",
    "        beta[len(sentence)-1][tag_idx] = 1\n",
    "    for word_idx in range(len(sentence)-1, -1, -1):\n",
    "        for tag_idx in range(0, len(tag_indexer)):\n",
    "            for next_tag_idx in range(0, len(tag_indexer)):\n",
    "                beta[word_idx][tag_idx] += beta[word_idx + 1][next_tag_idx] * scored_feature[next_tag_idx][tag_idx][word_idx]\n",
    "    return beta\n",
    "\n",
    "\n",
    "# Retrieves a word's index based on its count. If the word occurs only once, treat it as an \"UNK\" token\n",
    "# At test time, unknown words will be replaced by UNKs.\n",
    "def get_word_index(word_indexer, word_counter, word):\n",
    "    if word_counter.get_count(word) < 1.5:\n",
    "        return word_indexer.get_index(\"UNK\")\n",
    "    else:\n",
    "        return word_indexer.get_index(word)\n",
    "\n",
    "\n",
    "class FeatureBasedSequenceScorer(object):\n",
    "    def __init__(self, tag_indexer, feature_indexer, feature_weights):\n",
    "        self.tag_indexer = tag_indexer\n",
    "        self.feature_indexer = feature_indexer\n",
    "        self.feature_weights = feature_weights\n",
    "\n",
    "    def score_init(self, feature_cache, tag_idx):\n",
    "        return score_indexed_features(feature_cache[0][tag_idx], self.feature_weights)\n",
    "\n",
    "    def score_transition(self, feature_cache, prev_tag_idx, curr_tag_idx):\n",
    "        return 0\n",
    "\n",
    "    def score_emission(self, feature_cache, tag_idx, word_idx):\n",
    "        return score_indexed_features(feature_cache[word_idx][tag_idx], self.feature_weights)\n",
    "\n",
    "\n",
    "class CrfNerModel(object):\n",
    "    def __init__(self, tag_indexer, feature_indexer, feature_weights):\n",
    "        self.tag_indexer = tag_indexer\n",
    "        self.feature_indexer = feature_indexer\n",
    "        self.feature_weights = feature_weights\n",
    "\n",
    "    # Takes a LabeledSentence object and returns a new copy of that sentence with a set of chunks predicted by\n",
    "    # the CRF model. See BadNerModel for an example implementation\n",
    "    def decode(self, sentence):\n",
    "        feature_cache = [[[] for k in xrange(0, len(self.tag_indexer))] for j in xrange(0, len(sentence))]\n",
    "        for word_idx in range(0, len(sentence)):\n",
    "            for tag_idx in range(0, len(self.tag_indexer)):\n",
    "                feature_cache[word_idx][tag_idx] = extract_emission_features(sentence, word_idx, self.tag_indexer.get_object(tag_idx), self.feature_indexer, add_to_indexer=False)\n",
    "\n",
    "        # Viterbi\n",
    "        score = np.zeros((len(sentence), len(self.tag_indexer)))\n",
    "        back_pointers = np.ones((len(sentence), len(self.tag_indexer))) * -1\n",
    "        sequence_scorer = FeatureBasedSequenceScorer(self.tag_indexer, self.feature_indexer, self.feature_weights)\n",
    "        for word_idx in xrange(0, len(sentence)):\n",
    "            if word_idx == 0:\n",
    "                for tag_idx in xrange(0, len(self.tag_indexer)):\n",
    "                    tag = self.tag_indexer.get_object(tag_idx)\n",
    "                    if isI(tag):\n",
    "                        score[word_idx][tag_idx] = -np.inf\n",
    "                    else:    \n",
    "                        score[word_idx][tag_idx] = sequence_scorer.score_init(feature_cache, tag_idx)\n",
    "            else:\n",
    "                for curr_tag_idx in xrange(0, len(self.tag_indexer)):\n",
    "                    score[word_idx][curr_tag_idx] = -np.inf\n",
    "                    for prev_tag_idx in xrange(0, len(self.tag_indexer)):\n",
    "                        # TODO : did not prohibit the O-I transition at the last word\n",
    "                        curr_tag = self.tag_indexer.get_object(curr_tag_idx)\n",
    "                        prev_tag = self.tag_indexer.get_object(prev_tag_idx)\n",
    "                        if isO(prev_tag) and isI(curr_tag):\n",
    "                            continue\n",
    "                        if isI(curr_tag) and (get_tag_label(curr_tag) != get_tag_label(prev_tag)):\n",
    "                            continue\n",
    "                        curr_score = sequence_scorer.score_transition(feature_cache, prev_tag_idx, curr_tag_idx) + \\\n",
    "                                        sequence_scorer.score_emission(feature_cache, curr_tag_idx, word_idx) + score[word_idx-1][prev_tag_idx]\n",
    "                        if curr_score > score[word_idx][curr_tag_idx]:\n",
    "                            score[word_idx][curr_tag_idx] = curr_score\n",
    "                            back_pointers[word_idx][curr_tag_idx] = prev_tag_idx\n",
    "        max_score_idx = score.argmax(axis=1)[-1]\n",
    "        idx = max_score_idx\n",
    "        pred_tags = []\n",
    "        word_idx = len(sentence) - 1\n",
    "        while idx != -1 :\n",
    "            pred_tags.append(self.tag_indexer.get_object(idx))\n",
    "            idx = back_pointers[int(word_idx)][int(idx)]\n",
    "            word_idx -= 1\n",
    "        pred_tags.reverse()\n",
    "        return LabeledSentence(sentence.tokens, chunks_from_bio_tag_seq(pred_tags))\n",
    "\n",
    "# Trains a CrfNerModel on the given corpus of sentences.\n",
    "def train_crf_model(sentences, epochs, lr, weights_file=\"\", output_weights=\"\"):\n",
    "    tag_indexer = Indexer()\n",
    "    for sentence in sentences:\n",
    "        for tag in sentence.get_bio_tags():\n",
    "            tag_indexer.get_index(tag)\n",
    "    transition_mat = np.ones((len(tag_indexer), len(tag_indexer)))\n",
    "    for tag_idxa in range(0, len(tag_indexer)):\n",
    "        for tag_idxb in range(0, len(tag_indexer)):\n",
    "            tag_a = tag_indexer.get_object(tag_idxa)\n",
    "            tag_b = tag_indexer.get_object(tag_idxb)\n",
    "            if isI(tag_b) and (get_tag_label(tag_b) != get_tag_label(tag_a)):\n",
    "                transition_mat[tag_idxa][tag_idxb] = 0\n",
    "    print \"Extracting features\"\n",
    "    feature_indexer = Indexer()\n",
    "    # 4-d list indexed by sentence index, word index, tag index, feature index\n",
    "    feature_cache = [[[[] for k in xrange(0, len(tag_indexer))] for j in xrange(0, len(sentences[i]))] for i in xrange(0, len(sentences))]\n",
    "    for sentence_idx in xrange(0, len(sentences)):\n",
    "        if sentence_idx % 500 == 0:\n",
    "            print(\"Ex \" + repr(sentence_idx) + \"/\" + repr(len(sentences)))\n",
    "        for word_idx in xrange(0, len(sentences[sentence_idx])):\n",
    "            for tag_idx in xrange(0, len(tag_indexer)):\n",
    "                feature_cache[sentence_idx][word_idx][tag_idx] = extract_emission_features(sentences[sentence_idx], word_idx, tag_indexer.get_object(tag_idx), feature_indexer, add_to_indexer=True)\n",
    "    feature_weights = np.random.rand((len(feature_indexer)))\n",
    "    if weights_file != \"\":\n",
    "        feature_weights = np.load(weights_file)\n",
    "\n",
    "    print(\"Initital Statistics\")\n",
    "    model = CrfNerModel(tag_indexer, feature_indexer, feature_weights)\n",
    "    # TODO : currently using only emission features, also extend to transition features if possible\n",
    "    batch_size = 1\n",
    "    # training loop\n",
    "    for epoch in range(0, epochs):\n",
    "        print(\"Epoch %d\" % (epoch+1))\n",
    "        gradient = Counter()\n",
    "        for sentence_idx in range(0, len(sentences)):\n",
    "            if sentence_idx%500 == 0:\n",
    "                print('Training on ' + repr(sentence_idx))\n",
    "            log_marginal_probs = compute_log_marginals(sentences[sentence_idx], tag_indexer, feature_cache[sentence_idx], model.feature_weights)\n",
    "\n",
    "            for word_idx in range(0, len(sentences[sentence_idx])):\n",
    "                for tag_idx in range(0, len(tag_indexer)):\n",
    "                    gradient.increment_all(feature_cache[sentence_idx][word_idx][tag_idx], - np.exp(log_marginal_probs[word_idx][tag_idx]))\n",
    "                gold_tag = sentences[sentence_idx].get_bio_tags()[word_idx]\n",
    "                gold_tag_idx = tag_indexer.index_of(gold_tag)\n",
    "                gradient.increment_all(feature_cache[sentence_idx][word_idx][gold_tag_idx], 1.0)\n",
    "            if (sentence_idx+1) % batch_size == 0:\n",
    "                for weight_idx in gradient.keys():\n",
    "                    model.feature_weights[weight_idx] += (lr * gradient.get_count(weight_idx))/batch_size\n",
    "                gradient = Counter()\n",
    "    np.save(output_weights, model.feature_weights)\n",
    "    return model\n",
    "\n",
    "# TODO : implementation specific to emission features only, change forward and backward if adding transition features\n",
    "def compute_log_marginals(sentence, tag_indexer, sentence_feature_cache, feature_weights):\n",
    "    # find alpha -> forward pass\n",
    "    log_alpha = np.zeros((len(sentence), len(tag_indexer)))\n",
    "    for tag_idx in xrange(0, len(tag_indexer)):\n",
    "        log_alpha[0][tag_idx] = score_indexed_features(sentence_feature_cache[0][tag_idx], feature_weights)\n",
    "    for word_idx in xrange(1, len(sentence)):\n",
    "        for tag_idx in xrange(0, len(tag_indexer)):\n",
    "            log_alpha[word_idx][tag_idx] = -np.inf\n",
    "            for prev_tag_idx in xrange(0, len(tag_indexer)):\n",
    "                curr_tag = tag_indexer.get_object(tag_idx)\n",
    "                prev_tag = tag_indexer.get_object(prev_tag_idx)\n",
    "                if isI(curr_tag) and get_tag_label(curr_tag) != get_tag_label(prev_tag):\n",
    "                    continue\n",
    "                log_alpha[word_idx][tag_idx] = np.logaddexp(log_alpha[word_idx][tag_idx], \\\n",
    "                                                            log_alpha[word_idx - 1][prev_tag_idx] + \\\n",
    "                                                        score_indexed_features(sentence_feature_cache[word_idx][tag_idx], feature_weights))\n",
    "             \n",
    "            # log_alpha[word_idx][tag_idx] = scipy.misc.logsumexp(log_alpha[word_idx-1] + score_indexed_features(sentence_feature_cache[word_idx][tag_idx], feature_weights))\n",
    "    \n",
    "    # find beta -> backward pass\n",
    "    log_beta = np.zeros((len(sentence), len(tag_indexer)))\n",
    "    for word_idx in range(len(sentence)-2, -1, -1):\n",
    "        for tag_idx in range(0, len(tag_indexer)):\n",
    "            log_beta[word_idx][tag_idx] = -np.inf\n",
    "            for next_tag_idx in range(0, len(tag_indexer)):\n",
    "                curr_tag = tag_indexer.get_object(tag_idx)\n",
    "                next_tag = tag_indexer.get_object(next_tag_idx)\n",
    "                if isI(next_tag) and get_tag_label(curr_tag) != get_tag_label(next_tag):\n",
    "                    continue\n",
    "                log_beta[word_idx][tag_idx] = np.logaddexp(log_beta[word_idx][tag_idx], \\\n",
    "                                                        log_beta[word_idx + 1][next_tag_idx] + \\\n",
    "                                                        score_indexed_features(sentence_feature_cache[word_idx][next_tag_idx], feature_weights))\n",
    "            # tmp = np.apply_along_axis(score_indexed_features, 1, sentence_feature_cache[word_idx], feature_weights)\n",
    "            # log_beta[word_idx][tag_idx] = scipy.misc.logsumexp(log_beta[word_idx + 1] + tmp)\n",
    "\n",
    "    # marginal = alpha[word_idx][tag_idx] * beta[word_idx][tag_idx] / Sigma (alpha, beta)\n",
    "    log_marginal_probs = np.zeros((len(sentence), len(tag_indexer)))\n",
    "    log_marginal_probs = log_alpha + log_beta\n",
    "    # denom = np.apply_along_axis(scipy.misc.logsumexp, 1, log_marginal_probs)\n",
    "    # log_marginal_probs -= denom[:, None]\n",
    "    for word_idx in range(0, len(sentence)):\n",
    "        denom = -np.inf\n",
    "        for tag_idx in range(0, len(tag_indexer)):\n",
    "            denom = np.logaddexp(denom, log_marginal_probs[word_idx][tag_idx])\n",
    "        log_marginal_probs[word_idx] -= denom\n",
    "    return log_marginal_probs\n",
    "\n",
    "# Extracts emission features for tagging the word at word_index with tag.\n",
    "# add_to_indexer is a boolean variable indicating whether we should be expanding the indexer or not:\n",
    "# this should be True at train time (since we want to learn weights for all features) and False at\n",
    "# test time (to avoid creating any features we don't have weights for).\n",
    "def extract_emission_features(sentence, word_index, tag, feature_indexer, add_to_indexer):\n",
    "    feats = []\n",
    "    curr_word = sentence.tokens[word_index].word\n",
    "    # Lexical and POS features on this word, the previous, and the next (Word-1, Word0, Word1)\n",
    "    for idx_offset in xrange(-1, 2):\n",
    "        if word_index + idx_offset < 0:\n",
    "            active_word = \"<s>\"\n",
    "        elif word_index + idx_offset >= len(sentence):\n",
    "            active_word = \"</s>\"\n",
    "        else:\n",
    "            active_word = sentence.tokens[word_index + idx_offset].word\n",
    "        if word_index + idx_offset < 0:\n",
    "            active_pos = \"<S>\"\n",
    "        elif word_index + idx_offset >= len(sentence):\n",
    "            active_pos = \"</S>\"\n",
    "        else:\n",
    "            active_pos = sentence.tokens[word_index + idx_offset].pos\n",
    "        maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":Word\" + repr(idx_offset) + \"=\" + active_word)\n",
    "        maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":Pos\" + repr(idx_offset) + \"=\" + active_pos)\n",
    "    # Character n-grams of the current word\n",
    "    max_ngram_size = 3\n",
    "    for ngram_size in xrange(1, max_ngram_size+1):\n",
    "        start_ngram = curr_word[0:min(ngram_size, len(curr_word))]\n",
    "        maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":StartNgram=\" + start_ngram)\n",
    "        end_ngram = curr_word[max(0, len(curr_word) - ngram_size):]\n",
    "        maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":EndNgram=\" + end_ngram)\n",
    "    # Look at a few word shape features\n",
    "    maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":IsCap=\" + repr(curr_word[0].isupper()))\n",
    "    maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":IsAllCap=\" + repr(curr_word.isupper()))\n",
    "    maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":dashExists=\" + repr(\"-\" in curr_word))\n",
    "    # Compute word shape\n",
    "    new_word = []\n",
    "    for i in xrange(0, len(curr_word)):\n",
    "        if curr_word[i].isupper():\n",
    "            new_word += \"X\"\n",
    "        elif curr_word[i].islower():\n",
    "            new_word += \"x\"\n",
    "        elif curr_word[i].isdigit():\n",
    "            new_word += \"0\"\n",
    "        else:\n",
    "            new_word += \"?\"\n",
    "    maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":WordShape=\" + repr(new_word))\n",
    "    return np.asarray(feats, dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d3e7e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BadNerModel():\n",
    "    def __init__(self, words_to_tag_counters):\n",
    "        self.words_to_tag_counters = words_to_tag_counters\n",
    "\n",
    "    def decode(self, sentence):\n",
    "        pred_tags = []\n",
    "        for tok in sentence.tokens:\n",
    "            if self.words_to_tag_counters.has_key(tok.word):\n",
    "                pred_tags.append(self.words_to_tag_counters[tok.word].argmax())\n",
    "            else:\n",
    "                pred_tags.append(\"O\")\n",
    "        return LabeledSentence(sentence.tokens, chunks_from_bio_tag_seq(pred_tags))\n",
    "\n",
    "\n",
    "def train_bad_ner_model(training_set):\n",
    "    words_to_tag_counters = {}\n",
    "    for sentence in training_set:\n",
    "        tags = sentence.get_bio_tags()\n",
    "        for idx in xrange(0, len(sentence)):\n",
    "            word = sentence.tokens[idx].word\n",
    "            if not words_to_tag_counters.has_key(word):\n",
    "                words_to_tag_counters[word] = Counter()\n",
    "                words_to_tag_counters[word].increment_count(tags[idx], 1.0)\n",
    "    return BadNerModel(words_to_tag_counters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68b988e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features\n",
      "Ex 0/14934\n",
      "Ex 500/14934\n",
      "Ex 1000/14934\n",
      "Ex 1500/14934\n",
      "Ex 2000/14934\n",
      "Ex 2500/14934\n",
      "Ex 3000/14934\n",
      "Ex 3500/14934\n",
      "Ex 4000/14934\n",
      "Ex 4500/14934\n",
      "Ex 5000/14934\n",
      "Ex 5500/14934\n",
      "Ex 6000/14934\n",
      "Ex 6500/14934\n",
      "Ex 7000/14934\n",
      "Ex 7500/14934\n",
      "Ex 8000/14934\n",
      "Ex 8500/14934\n",
      "Ex 9000/14934\n",
      "Ex 9500/14934\n",
      "Ex 10000/14934\n",
      "Ex 10500/14934\n",
      "Ex 11000/14934\n",
      "Ex 11500/14934\n",
      "Ex 12000/14934\n",
      "Ex 12500/14934\n",
      "Ex 13000/14934\n",
      "Ex 13500/14934\n",
      "Ex 14000/14934\n",
      "Ex 14500/14934\n",
      "Initital Statistics\n",
      "Epoch 1\n",
      "Training on 0\n",
      "Training on 500\n",
      "Training on 1000\n",
      "Training on 1500\n",
      "Training on 2000\n",
      "Training on 2500\n",
      "Training on 3000\n",
      "Training on 3500\n",
      "Training on 4000\n",
      "Training on 4500\n",
      "Training on 5000\n",
      "Training on 5500\n",
      "Training on 6000\n",
      "Training on 6500\n",
      "Training on 7000\n",
      "Training on 7500\n",
      "Training on 8000\n",
      "Training on 8500\n",
      "Training on 9000\n",
      "Training on 9500\n",
      "Training on 10000\n",
      "Training on 10500\n",
      "Training on 11000\n",
      "Training on 11500\n",
      "Training on 12000\n",
      "Training on 12500\n",
      "Training on 13000\n",
      "Training on 13500\n",
      "Training on 14000\n",
      "Training on 14500\n",
      "Labeled F1: 72.74, precision: 4149/5661 = 73.29, recall: 4149/5746 = 72.21\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description='NER tagging system')\n",
    "#     parser.add_argument('--model', dest='model', type=str, default='BAD')\n",
    "#     parser.add_argument('--preW', dest='preW', type=str, default='')\n",
    "#     parser.add_argument('--outW', dest='outW', type=str, default='')\n",
    "#     parser.add_argument('-e', '--epochs', dest='epochs', type=int, default=10)\n",
    "#     parser.add_argument('-l', '--language', dest='lang', type=str, default='eng')\n",
    "#     parser.add_argument('-lr', '--lr', dest='lr', type=float, default=0.1)\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # Load the training and test data\n",
    "    train = read_data(\"data/eng.train\")\n",
    "    dev = read_data(\"data/eng.testa\")\n",
    "    # Here's a few sentences...\n",
    "    # print \"Examples of sentences:\"\n",
    "    # print str(dev[1])\n",
    "    # print str(dev[3])\n",
    "    # print str(dev[5])\n",
    "    system_to_run = 'CRF'\n",
    "    # Set to True when you're ready to run your CRF on the test set to produce the final output\n",
    "    run_on_test = True\n",
    "    # Train our model\n",
    "    if system_to_run == \"BAD\":\n",
    "        bad_model = train_bad_ner_model(train)\n",
    "        dev_decoded = [bad_model.decode(test_ex) for test_ex in dev]\n",
    "    elif system_to_run == \"HMM\":\n",
    "        hmm_model = train_hmm_model(train)\n",
    "        dev_decoded = [hmm_model.decode(test_ex) for test_ex in dev]\n",
    "    elif system_to_run == \"CRF\":\n",
    "        crf_model = train_crf_model(train, 1, 0.1, weights_file='', output_weights='')\n",
    "        dev_decoded = [crf_model.decode(test_ex) for test_ex in dev]\n",
    "        if run_on_test:\n",
    "            test = read_data(\"data/eng.testb.blind\")\n",
    "            test_decoded = [crf_model.decode(test_ex) for test_ex in test]\n",
    "            print_output(test_decoded, \"eng.testb.out\")\n",
    "    else:\n",
    "        raise Exception(\"Pass in either BAD, HMM, or CRF to run the appropriate system\")\n",
    "    # Print the evaluation statistics\n",
    "    print_evaluation(dev, dev_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7837207",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8632d76e6ee6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#     parser.add_argument('-lr', '--lr', dest='lr', type=float, default=0.1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#     args = parser.parse_args()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mword_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'word_idx' is not defined"
     ]
    }
   ],
   "source": [
    "#     parser = argparse.ArgumentParser(description='NER tagging system')\n",
    "#     parser.add_argument('--model', dest='model', type=str, default='BAD')\n",
    "#     parser.add_argument('--preW', dest='preW', type=str, default='')\n",
    "#     parser.add_argument('--outW', dest='outW', type=str, default='')\n",
    "#     parser.add_argument('-e', '--epochs', dest='epochs', type=int, default=10)\n",
    "#     parser.add_argument('-l', '--language', dest='lang', type=str, default='eng')\n",
    "#     parser.add_argument('-lr', '--lr', dest='lr', type=float, default=0.1)\n",
    "#     args = parser.parse_args()\n",
    "word_idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
